import os
import json
from tqdm import tqdm
from src.utils.config import DATA_RAW, DATA_PROCESSED, DATA_OUTPUT, CHUNK_SIZE, OVERLAP, MODEL_EXTRACTOR, MODEL_WRITER, OLLAMA_URL
from src.core.text_cleaner import clean_transcript
from src.utils.text_processing import smart_split_text
from src.agents.extractor import extract_knowledge
from src.agents.writer import generate_chapter
from src.core.llm_engine import unload_model
from src.utils.validator import verify_url

def run_pipeline(input_path: str, output_dir: str = DATA_OUTPUT, topic: str = "Narzƒôdzia OSINT, Krypto i Techniki ≈öledcze"):
    """
    Uruchamia pipeline generowania podrƒôcznika.
    
    Args:
        input_path (str): Absolutna ≈õcie≈ºka do pliku wej≈õciowego (transkrypcji).
        output_dir (str): Katalog zapisu wynik√≥w.
        topic (str): Temat rozdzia≈Çu/podrƒôcznika.
    """
    if not os.path.exists(input_path):
        print(f"B≈ÇƒÖd: Nie znaleziono pliku {input_path}")
        return

    filename = os.path.basename(input_path)
    print(f"üöÄ Rozpoczynam przetwarzanie: {filename}")
    print(f"üìö Temat: {topic}")

    # 1. Wczytywanie i czyszczenie
    with open(input_path, 'r', encoding='utf-8') as f:
        raw_text = f.read()
    
    print("üßπ Czyszczenie tekstu...")
    clean_text = clean_transcript(raw_text)
    
    # U≈ºycie nowego splittera
    chunks = smart_split_text(clean_text, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)
    print(f"üì¶ Podzielono na {len(chunks)} fragment√≥w (Chunk size: {CHUNK_SIZE}).")

    # 2. Mapowanie (Ekstrakcja Qwenem)
    knowledge_base = []
    print(f"\nüïµÔ∏è Ekstrakcja wiedzy (Model: {MODEL_EXTRACTOR})...")
    
    for i, chunk in enumerate(tqdm(chunks)):
        # Extract returns KnowledgeGraph object
        graph = extract_knowledge(chunk)
        
        # Walidacja URLi w narzƒôdziach
        valid_tools = []
        for tool in graph.tools:
            if tool.url:
                if verify_url(tool.url):
                    valid_tools.append(tool)
                else:
                    print(f"\n‚ö†Ô∏è Wykryto halucynacjƒô URL: {tool.url} (Narzƒôdzie: {tool.name})")
            else:
                valid_tools.append(tool)
        
        graph.tools = valid_tools
        
        # Konwersja do dict dla serializacji JSON
        knowledge_base.append(graph.model_dump())
        
        # Backup co 5 fragment√≥w
        if i % 5 == 0:
            with open(os.path.join(DATA_PROCESSED, "knowledge_backup.json"), 'w', encoding='utf-8') as f:
                json.dump(knowledge_base, f, ensure_ascii=False, indent=2)

    # Zapisz pe≈ÇnƒÖ bazƒô wiedzy
    kb_path = os.path.join(DATA_PROCESSED, f"{filename}_kb.json")
    with open(kb_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Baza wiedzy zapisana w: {kb_path}")

    # Zwalnianie modelu Extractora przed za≈Çadowaniem Writera
    print("üßπ Zwalnianie pamiƒôci VRAM...")
    unload_model(MODEL_EXTRACTOR)

    # 3. Redukcja (Pisanie Bielikiem)
    print(f"\n‚úçÔ∏è Pisanie podrƒôcznika (Model: {MODEL_WRITER})...")
    
    final_content = f"# Podrƒôcznik: {topic}\n\n"
    
    # Generujemy rozdzia≈Ç
    # Note: generate_chapter expects list of dicts, which matches knowledge_base structure now
    chapter_tools = generate_chapter(topic, knowledge_base)
    final_content += chapter_tools
    
    # Zapis
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"Podrecznik_{filename.replace('.txt', '.md')}")
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(final_content)
        
    print(f"\nüéâ SUKCES! Gotowy plik: {output_path}")

if __name__ == "__main__":
    # Podaj nazwƒô pliku, kt√≥ry wrzuci≈Çe≈õ do data/raw/
    # Domy≈õlnie szukamy pierwszego pliku .txt w folderze raw je≈õli nie podano
    files = [f for f in os.listdir(DATA_RAW) if f.endswith('.txt')]
    if files:
        TARGET_FILE = os.path.join(DATA_RAW, files[0])
        run_pipeline(TARGET_FILE)
    else:
        print("Brak plik√≥w .txt w data/raw")
