import os
import json
import shutil
from tqdm import tqdm
from src.utils.config import (
    DATA_RAW, DATA_PROCESSED, DATA_OUTPUT, CHUNK_SIZE, OVERLAP,
    MODEL_EXTRACTOR, MODEL_WRITER, OBSIDIAN_VAULT_PATH,
    OBSIDIAN_EXPORT_ENABLED, OBSIDIAN_SUBFOLDER
)
from src.core.text_cleaner import clean_transcript
from src.utils.text_processing import smart_split_text
from src.core.transcriber import Transcriber
from src.core.gpu_manager import clear_gpu_memory
from src.agents.extractor import KnowledgeExtractor
from src.agents.writer import ReportWriter
from src.core.llm_engine import unload_model


def run_pipeline(input_path: str, output_dir: str = DATA_OUTPUT, topic: str = "Narzƒôdzia OSINT, Krypto i Techniki ≈öledcze", whisper_model: str = "large-v3"):
    if not os.path.exists(input_path):
        print(f"B≈ÇƒÖd: Nie znaleziono pliku {input_path}")
        return

    filename = os.path.basename(input_path)
    print(f"\nüöÄ {'='*60}")
    print(f"üöÄ ROZPOCZYNAM PRZETWARZANIE: {filename}")
    print(f"üöÄ {'='*60}")

    # --- KROK 1: Transkrypcja (je≈õli plik nie jest .txt) ---
    txt_path = input_path
    if not input_path.endswith('.txt'):
        print(f"\nüéôÔ∏è [KROK 1] Transkrypcja Whisper (Model: {whisper_model})...")
        transcriber = Transcriber(logger=None, stop_event=None, progress_callback=lambda p, s: None)
        
        # Miejscowa definicja mock-loggera i stop_event dla Transcribera
        class SimpleLogger:
            def log(self, m): print(f"  [Whisper] {m}")
        
        class SimpleStopEvent:
            def is_set(self): return False

        transcriber.logger = SimpleLogger()
        transcriber.stop_event = SimpleStopEvent()
        transcriber.progress_callback = lambda p, s: None

        segments, info = transcriber.transcribe_video(input_path, language=None, model_size=whisper_model)
        txt_path, _ = transcriber.save_transcription(segments, info, input_path, output_format="txt", language=None)
        
        # --- KROK 1.5: WYMUSZONE CZYSZCZENIE VRAM ---
        print("\nüßπ [CZYSZCZENIE] Zwalnianie VRAM po Whisperze...")
        del transcriber
        clear_gpu_memory(verbose=True)
        print("‚úÖ VRAM gotowy na LLM.")

    # 2. Wczytywanie i czyszczenie
    with open(txt_path, 'r', encoding='utf-8') as f:
        raw_text = f.read()
    
    clean_text = clean_transcript(raw_text)
    chunks = smart_split_text(clean_text, chunk_size=CHUNK_SIZE, chunk_overlap=OVERLAP)
    print(f"\nüì¶ Podzielono na {len(chunks)} fragment√≥w.")

    # 3. Mapowanie (Ekstrakcja)
    knowledge_base = []
    failed_chunks = 0
    stats = {
        "tools": 0,
        "concepts": 0,
        "topics": 0,
        "tips": 0
    }
    
    print(f"\nüïµÔ∏è [KROK 2] Ekstrakcja wiedzy (Model: {MODEL_EXTRACTOR}, num_ctx: 4096)...")
    
    extractor = KnowledgeExtractor()
    total_chunks = len(chunks)
    for i, chunk in enumerate(tqdm(chunks)):
        # Oznaczanie fragmentu (Part X (Y%))
        progress_pct = int(((i + 1) / total_chunks) * 100)
        time_tag = f"Part {i+1} ({progress_pct}%)"
        
        graph = extractor.extract_knowledge(chunk, chunk_id=time_tag)
        
        # Wykrywanie cichego b≈Çƒôdu
        is_empty_graph = not any([graph.topics, graph.tools, graph.key_concepts, graph.tips])
        
        if is_empty_graph:
            if len(chunk) > 100:
                failed_chunks += 1
                print(f"\n‚ö†Ô∏è [OSTRZE≈ªENIE] Fragment {time_tag} zwr√≥ci≈Ç puste dane.")
        
        stats["tools"] += len(graph.tools)
        stats["concepts"] += len(graph.key_concepts)
        stats["topics"] += len(graph.topics)
        stats["tips"] += len(graph.tips)
        
        knowledge_base.append(graph.model_dump())
        
        # Backup co 5 fragment√≥w
        if i % 5 == 0:
            os.makedirs(DATA_PROCESSED, exist_ok=True)
            with open(os.path.join(DATA_PROCESSED, "knowledge_backup.json"), 'w', encoding='utf-8') as f:
                json.dump(knowledge_base, f, ensure_ascii=False, indent=2)

    # Raport ko≈Ñcowy ekstrakcji
    print(f"\nüìä RAPORT EKSTRAKCJI:")
    print(f"   - Przetworzono: {len(chunks)} fragment√≥w")
    print(f"   - Znaleziono narzƒôdzi: {stats['tools']}")
    print(f"   - Zdefiniowano pojƒôƒá: {stats['concepts']}")
    print(f"   - Wykryto b≈Çƒôd√≥w: {failed_chunks}")
    if failed_chunks > 0:
        print(f"   üö® UWAGA: Brakuje {failed_chunks} fragment√≥w wiedzy.")

    # Zapis bazy wiedzy
    kb_name = os.path.basename(txt_path)
    kb_path = os.path.join(DATA_PROCESSED, f"{kb_name}_kb.json")
    os.makedirs(DATA_PROCESSED, exist_ok=True)
    with open(kb_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False, indent=2)

    unload_model(MODEL_EXTRACTOR)

    # 4. Redukcja (Pisanie)
    if not knowledge_base or (failed_chunks == len(chunks)):
        print("‚ùå B≈ÇƒÖd krytyczny: Brak danych do napisania podrƒôcznika.")
        return

    print(f"\n‚úçÔ∏è [KROK 3] Pisanie tre≈õci (Model: {MODEL_WRITER})...")
    writer = ReportWriter()
    
    # Generujemy tre≈õƒá (bez tag√≥w na razie)
    content_only = writer.generate_chapter(topic, knowledge_base, mode="deep_dive", tags=[])
    
    # --- CZYSZCZENIE VRAM po Pisarzu ---
    print("\nüßπ [CZYSZCZENIE] Zwalnianie VRAM po Bieliku...")
    from src.core.llm_engine import unload_model
    unload_model(MODEL_WRITER)
    
    # 5. Tagowanie (Nowy Krok)
    print(f"\nüè∑Ô∏è [KROK 4] Generowanie tag√≥w (Model: Qwen)...")
    from src.agents.tagger import TaggerAgent
    tagger = TaggerAgent()
    tags = tagger.generate_tags(content_only)
    print(f"‚úÖ Wygenerowano tagi: {', '.join(tags)}")
    
    # --- CZYSZCZENIE VRAM po Taggerze ---
    unload_model("qwen2.5:7b") # Zak≈ÇadajƒÖc ≈ºe to extractor
    
    # 6. Sk≈Çadanie finalne
    # Ponownie u≈ºywamy ReportWriter tylko do z≈Ço≈ºenia YAML (bez ponownego generowania tre≈õci)
    # Tu ma≈Çy hack: ReportWriter.generate_chapter generuje tre≈õƒá...
    # Musimy zaktualizowaƒá frontmatter w content_only lub dodaƒá metodƒô do ReportWriter.
    
    # Poprawka: Zaktualizujmy frontmatter mechanicznie lub dodajmy metodƒô do writer.py
    # Zr√≥bmy to porzƒÖdnie w ReportWriter.
    
    final_output = content_only.replace("tags: []", f"tags: {tags}")
    
    final_content = f"# Podrƒôcznik: {topic}\n\n{final_output}"
    
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"Podrecznik_{filename.split('.')[0]}.md")
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(final_content)

    print(f"\nüéâ SUKCES! Plik zapisany: {output_path}")

    # Eksport do Obsidian Vault
    if OBSIDIAN_EXPORT_ENABLED:
        export_to_obsidian(output_path)


if __name__ == "__main__":
    # Obs≈Çuga wielu plik√≥w i r√≥≈ºnych format√≥w
    supported_extensions = ('.txt', '.mp3', '.mp4', '.m4a', '.wav')
    files = [f for f in os.listdir(DATA_RAW) if f.lower().endswith(supported_extensions)]
    
    if files:
        print(f"Found {len(files)} files to process in {DATA_RAW}")
        for file in files:
            try:
                run_pipeline(os.path.join(DATA_RAW, file))
            except Exception as e:
                print(f"‚ùå Error processing {file}: {e}")
    else:
        print(f"Brak wspieranych plik√≥w w {DATA_RAW}")

