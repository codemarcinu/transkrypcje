# System Generowania PodrÄ™cznikÃ³w AI (Map-Reduce)

Projekt przeksztaÅ‚ca surowe transkrypcje wideo w profesjonalne, ustrukturyzowane rozdziaÅ‚y podrÄ™cznikÃ³w IT, wykorzystujÄ…c architekturÄ™ **Map-Reduce** i lokalne modele LLM.

## ğŸ—ï¸ Architektura

System dziaÅ‚a w trzech fazach:

1.  **Pozyskiwanie i Transkrypcja**
    - **YouTube**: Automatyczne pobieranie wideo/audio.
    - **Optymalizacja**: System automatycznie wykrywa i pobiera napisy z YouTube (PL/EN), co pozwala na ominiÄ™cie procesu transkrypcji i natychmiastowe przejÅ›cie do analizy.
    - **Whisper**: JeÅ›li napisy nie sÄ… dostÄ™pne, system wykorzystuje modele **Faster-Whisper** do lokalnej transkrypcji z wykorzystaniem GPU.

2.  **Ekstrakcja Wiedzy (Map)**
    - **Agent**: `Extractor` (oparty na **Qwen 2.5 14B**)
    - **Zadanie**: Analizuje tekst fragment po fragmencie, wyciÄ…gajÄ…c kluczowe informacje, techniki i pojÄ™cia.
    - **Wynik**: Baza wiedzy w formacie JSON (`data/processed/`).

3.  **Generowanie TreÅ›ci (Reduce & PKM)**
    - **Agent**: `Writer` (oparty na **Bielik 11B v3**)
    - **Zadanie**: Agreguje zebranÄ… wiedzÄ™ i pisze spÃ³jny rozdziaÅ‚ w formacie **Obsidian Markdown**.
    - **Cechy**: 
        - **YAML Frontmatter**: Automatyczne metadane (tags, status).
        - **Wikilinks**: Linkowanie narzÄ™dzi i pojÄ™Ä‡ `[[NarzÄ™dzie]]`.
        - **Indeks Å¹rÃ³dÅ‚owy**: Åšledzenie pochodzenia wiedzy we fragmentach transkrypcji.

4.  **ZarzÄ…dzanie Zasobami & StabilnoÅ›Ä‡**
    - **Retry Logic**: System automatycznie ponawia bÅ‚Ä™dy ekstrakcji.
    - **VRAM Optimization**: Wymuszone czyszczenie pamiÄ™ci GPU (`gc` + `torch.cuda.empty_cache()`) dla stabilnej pracy na kartach 12GB.

## ğŸ“‚ Struktura KatalogÃ³w

```text
transkrypcje/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Tu wrzucasz pliki .txt (np. "NarzÄ™dziownik...")
â”‚   â”œâ”€â”€ processed/           # Tu lÄ…dujÄ… JSON-y z wiedzÄ… (backup co 5 chunkÃ³w)
â”‚   â””â”€â”€ output/              # Gotowe rozdziaÅ‚y .md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # Logika agentÃ³w (Extractor: Qwen, Writer: Bielik)
â”‚   â”œâ”€â”€ core/                # Silnik LLM (Ollama wrapper) i czyszczenie tekstu
â”‚   â””â”€â”€ utils/               # Konfiguracja (Å›cieÅ¼ki, nazwy modeli)
â”œâ”€â”€ main_pipeline.py         # Skrypt uruchomieniowy
â”œâ”€â”€ Modelfile                # Definicja optymalizacji modelu Bielik
â””â”€â”€ requirements.txt         # ZaleÅ¼noÅ›ci Python
```

## ğŸš€ Instalacja i Uruchomienie

### 1. Wymagania
*   **Ollama** zainstalowana i dziaÅ‚ajÄ…ca.
*   **Python 3.10+**.
*   **GPU**: Zalecane min. 12GB VRAM (modele Å‚adowane sÄ… sekwencyjnie).

### 2. Przygotowanie Modeli
Pobierz Qwena i zbuduj zoptymalizowanego Bielika:

```bash
ollama pull qwen2.5:14b
ollama create bielik-writer -f Modelfile
```

### 3. Instalacja ZaleÅ¼noÅ›ci
```bash
# WewnÄ…trz venv
pip install -r requirements.txt
```

### 4. Uruchomienie (Windows)

Po prostu kliknij dwukrotnie plik:
`run_app.bat`

> *Skrypt automatycznie aktywuje Å›rodowisko i otworzy panel w przeglÄ…darce.*

### 5. Korzystanie
- Wybierz plik transkrypcji z listy po lewej.
- Temat wypeÅ‚ni siÄ™ automatycznie â€“ moÅ¼esz go zmieniÄ‡.
- Kliknij **"Generuj Notatki"**.
- Wynik zobaczysz od razu pod przyciskiem.
- JeÅ›li masz skonfigurowany **Obsidian Vault**, moÅ¼esz wysÅ‚aÄ‡ notatkÄ™ jednym klikniÄ™ciem.

> [!TIP]
> JeÅ›li system zwolni lub zauwaÅ¼ysz wysokie zuÅ¼ycie VRAM, uÅ¼yj przycisku **"Zwolnij VRAM"** w bocznym panelu.

> [!TIP]
> Wszystkie techniczne opcje (wybÃ³r modelu, jÄ™zyka, folderÃ³w) zostaÅ‚y ukryte w zakÅ‚adce **"âš™ï¸ Ustawienia Zaawansowane"** w bocznym panelu, aby interfejs pozostawaÅ‚ przejrzysty.

> [!NOTE]
> Oryginalny interfejs Tkinter zostaÅ‚ przeniesiony do `src/gui/legacy/` i moÅ¼na go uruchomiÄ‡ za pomocÄ… `run_legacy_gui.bat` (niepolecane).

### 6. Uruchomienie (CLI)
1.  WrzuÄ‡ plik transkrypcji do `data/raw/` (lub uÅ¼yj istniejÄ…cego w `data/output/`).
2.  Uruchom pipeline:
```bash
python main_pipeline.py
```

## ğŸ’¡ Customizacja

*   **Zmiana Modeli**: Edytuj `src/utils/config.py`.
*   **Zmiana Prompta**:
    *   Prompt ekstrakcji (Qwen): `src/agents/extractor.py`
    *   Prompt pisania (Bielik): `src/agents/writer.py`
    *   System Prompt Bielika: `Modelfile` (wymaga przebudowania modelu `ollama create ...`).

## âš ï¸ RozwiÄ…zywanie problemÃ³w

*   **PÄ™tle w tekÅ›cie ("i tak dalej")**: Upewnij siÄ™, Å¼e uÅ¼ywasz modelu `bielik-writer`, ktÃ³ry ma ustawione `repetition_penalty`.
*   **BÅ‚Ä™dy JSON**: Logika w `llm_engine.py` automatycznie czyÅ›ci Markdown, ale w razie problemÃ³w sprawdÅº surowe odpowiedzi w logach.
